{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70d3240e",
   "metadata": {},
   "source": [
    "### General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6065c67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /scratch/aht324/envs_dirs/env_pytorch/lib/python3.8/site-packages (3.5.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /scratch/aht324/envs_dirs/env_pytorch/lib/python3.8/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /scratch/aht324/envs_dirs/env_pytorch/lib/python3.8/site-packages (from matplotlib) (8.4.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /scratch/aht324/envs_dirs/env_pytorch/lib/python3.8/site-packages (from matplotlib) (4.28.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /scratch/aht324/envs_dirs/env_pytorch/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /scratch/aht324/envs_dirs/env_pytorch/lib/python3.8/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /scratch/aht324/envs_dirs/env_pytorch/lib/python3.8/site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: setuptools-scm>=4 in /scratch/aht324/envs_dirs/env_pytorch/lib/python3.8/site-packages (from matplotlib) (6.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /scratch/aht324/envs_dirs/env_pytorch/lib/python3.8/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /scratch/aht324/envs_dirs/env_pytorch/lib/python3.8/site-packages (from matplotlib) (1.21.2)\n",
      "Requirement already satisfied: six>=1.5 in /scratch/aht324/envs_dirs/env_pytorch/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /scratch/aht324/envs_dirs/env_pytorch/lib/python3.8/site-packages (from setuptools-scm>=4->matplotlib) (1.2.2)\n",
      "Requirement already satisfied: setuptools in /scratch/aht324/envs_dirs/env_pytorch/lib/python3.8/site-packages (from setuptools-scm>=4->matplotlib) (58.0.4)\n",
      "Requirement already satisfied: tqdm in /scratch/aht324/envs_dirs/env_pytorch/lib/python3.8/site-packages (4.62.3)\n",
      "Requirement already satisfied: sklearn in /scratch/aht324/envs_dirs/env_pytorch/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /scratch/aht324/envs_dirs/env_pytorch/lib/python3.8/site-packages (from sklearn) (1.0.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /scratch/aht324/envs_dirs/env_pytorch/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /scratch/aht324/envs_dirs/env_pytorch/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /scratch/aht324/envs_dirs/env_pytorch/lib/python3.8/site-packages (from scikit-learn->sklearn) (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /scratch/aht324/envs_dirs/env_pytorch/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.21.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "!pip install tqdm\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9166debd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn \n",
    "import torch.nn.init as init\n",
    "from IPython.display import Image \n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import pickle\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "seed = 12345\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b8941b",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b5cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e574be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b031eb97",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ab8b198",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pickle.load(open('okcupid_train.pkl', 'rb'))\n",
    "val = pickle.load(open('okcupid_val.pkl', 'rb'))\n",
    "test = pickle.load(open('okcupid_test.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b99ba5",
   "metadata": {},
   "source": [
    "### Dataset to handle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f058c9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7240"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5425003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        #tokenizers usually do the tokenization and numericalization in one step, use encode to get word->index and  \n",
    "        # decode to get index->word\n",
    "        self.tokenizer = GPT2TokenizerFast.from_pretrained('distilgpt2')\n",
    "        self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.batches = []\n",
    "        \n",
    "        for i in range(int(len(self.dataset) / batch_size)):\n",
    "            batch = self.dataset[i * self.batch_size : (i + 1) * self.batch_size]\n",
    "            self.batches += [self.tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")]\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        return self.batches[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90093658",
   "metadata": {},
   "source": [
    "### Preparing the data for training. BATCH SIZE is specified here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "145cd9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12f49101",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = GPTDataset(train, batch_size)\n",
    "val_dataset = GPTDataset(val, batch_size)\n",
    "test_dataset = GPTDataset(test, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455bdccf",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c118312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, optimizer, scheduler, train_dataset, device, epoch=5, \n",
    "               val_dataset=None, save_model_at_end = False):   \n",
    "    output_dir = '/model/'\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for t in tqdm(range(epoch)):\n",
    "        batches = 0 \n",
    "        total = 0\n",
    "        model.train()       \n",
    "        total_loss = 0\n",
    "        for batch_idx, x in tqdm(list(enumerate(train_dataset))):\n",
    "            batches += 1\n",
    "            x = x.to(device)\n",
    "            \n",
    "            output = model(**x, labels=x['input_ids'])\n",
    "            \n",
    "            loss = output[0]\n",
    "            total_loss += loss.sum().detach().item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step(loss.sum().detach().item())\n",
    "            \n",
    "        train_losses += [total_loss / batches]\n",
    "        \n",
    "        if val_dataset is not None:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                total_loss = 0\n",
    "                batches = 0 \n",
    "                total = 0\n",
    "                for batch_idx, x in enumerate(val_dataset):\n",
    "                    batches += 1\n",
    "                    x = x.to(device)\n",
    "\n",
    "                    output = model(**x, labels=x['input_ids'])\n",
    "                    loss = output[0]\n",
    "                    total_loss += loss.sum().detach().item()\n",
    "\n",
    "                val_losses += [total_loss / batches]\n",
    "                \n",
    "        print(\"[EPOCH]: %i, [TRAIN LOSS]: %.6f\" % (t, train_losses[-1]))\n",
    "        if val_dataset is not None:\n",
    "            print(\"[EPOCH]: %i, [VAL LOSS]: %.6f\" % (t, val_losses[-1]))\n",
    "    #change- return model, and save on last epoch\n",
    "    if save_model_at_end:\n",
    "        torch.save(model.state_dict(), os.path.join(output_dir, \"model.pt\"))\n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a38cc9",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8865007d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114f8a0e3fa943df8a134d56acfbfdfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d4938f0994d427295b9c3c45db8c174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "#model = model.cuda()\n",
    "model.resize_token_embeddings(len(train_dataset.tokenizer))\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "device = torch.device('cpu')\n",
    "trained_model, train_losses, val_losses = train_loop(model=model, optimizer=optimizer,\n",
    "                                      scheduler=scheduler, \n",
    "                                      train_dataset=train_dataset, \n",
    "                                      device=device, epoch=1, val_dataset=val_dataset, save_model_at_end = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bafc055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, entry_count=1, entry_length=30, top_p =0.8, temperature = 1.,):\n",
    "    model.eval()\n",
    "    generated_num = 0\n",
    "    generated_list = []\n",
    "\n",
    "    filter_value = -float(\"Inf\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for entry_idx in trange(entry_count):\n",
    "\n",
    "            entry_finished = False\n",
    "            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "\n",
    "            for i in range(entry_length):\n",
    "                outputs = model(generated, labels=generated)\n",
    "                loss, logits = outputs[:2]\n",
    "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                    ..., :-1\n",
    "                ].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = filter_value\n",
    "\n",
    "                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "                generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "                if next_token in tokenizer.encode(\"[PAD]\"):\n",
    "                    entry_finished = True\n",
    "\n",
    "                if entry_finished:\n",
    "\n",
    "                    generated_num = generated_num + 1\n",
    "\n",
    "                    output_list = list(generated.squeeze().numpy())\n",
    "                    output_text = tokenizer.decode(output_list)\n",
    "                    generated_list.append(output_text)\n",
    "                    break\n",
    "            \n",
    "            if not entry_finished:\n",
    "                output_list = list(generated.squeeze().numpy())\n",
    "                output_text = f\"{tokenizer.decode(output_list)}[PAD]\" \n",
    "                generated_list.append(output_text)\n",
    "                \n",
    "    return generated_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681eef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generation(test_data):\n",
    "    #testdata has to be a list, use test and not test_dataset\n",
    "    generated_desc = []\n",
    "    for i in range(len(test_data)):\n",
    "        x = generate(model.to('cpu'), tokenizer, test_data[i], entry_count=1)\n",
    "        generated_desc.append(x)\n",
    "    return generated_desc\n",
    "\n",
    "#Run the functions to generate the descriptions\n",
    "generated = text_generation(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e8b875",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
